# Отчёт по лабораторной работе №2: Обобщенная передача от всех одному (gather)
---

Студент: Цыплаков Кирилл Александрович, группа: 3823Б1ПР2\
Технологии: SEQ, MPI\
Вариант: 5

---

## 1. Введение 

В данной работе необходимо реализовать обобщенную передачу от всех одному (gather),
используя только функции Send и Recv. Работа должна иметь тот же прототип, что и оригинальная
MPI функция. Программа должна уметь выполнять пересылку массивов типа ```integer, double, float```.
Также во всех операциях передача должна выполняться с использованием "дерева" процессов.

---
## 2. Постановка задачи 

На вход подаётся локальный вектор данных, размещённый на каждом процессе параллельной программы, а также номер процесса root, на который необходимо собрать данные. Требуется реализовать операцию обобщённой передачи данных от всех процессов одному (gather), в результате которой локальные векторы всех процессов объединяются в один общий вектор на процессе с номером root.

Результатом работы алгоритма является вектор, содержащий данные всех процессов, 
упорядоченные в соответствии с их номерами процессов, при этом данные процесса с 
номером ```r``` должны занимать позиции
```[r * local_size, (r + 1) * local_size]``` результирующего вектора.

### Дополнительные требования и ограничения:

- Алгоритм должен поддерживать передачу массивов типов ```MPI_INT```, 
```MPI_FLOAT``` и ```MPI_DOUBLE```;
- Реализация параллельного алгоритма должна использовать только функции точечной 
передачи сообщений ```MPI_Send``` и ```MPI_Recv```, без применения коллективных 
операций MPI;
- Передача данных в параллельной версии должна 
выполняться с использованием древовидной схемы взаимодействия процессов, 
обеспечивающей логарифмическую сложность по числу процессов;
- Алгоритм должен корректно работать для произвольного номера корневого 
процесса ```root```;
- Для одинаковых входных данных последовательная и 
параллельная реализации должны возвращать одинаковый результат;
- Параллельная реализация должна корректно работать 
при любом количестве процессов и размерах передаваемых данных.

---

## 3. Базовый алгоритм

Базовый алгоритм заключается в 
реализации операции обобщённой передачи данных от всех процессов одному (gather). 
Каждый процесс располагает локальным вектором одинакового размера, который необходимо 
передать на корневой процесс ```root```. В результате работы алгоритма на процессе ```root``` 
формируется результирующий вектор, содержащий последовательно объединённые данные 
всех процессов в порядке их номеров.

### Последовательная версия алгоритма

В последовательной версии предполагается выполнение программы на одном процессе. 
В этом случае операция сбора данных сводится к возврату входного вектора, если 
номер корневого процесса равен нулю.

В конструкторе принимается входной вектор и номер корневого процесса, 
выходной вектор инициализируется пустым:

```
TsyplakovKFromAllToOneSEQ::TsyplakovKFromAllToOneSEQ(const InTypeSEQ& in) {
  SetTypeOfTask(GetStaticTypeOfTask());
  GetInput() = in;
}
```

В методе ```RunImpl``` выполняется проверка номера корневого процесса. 
Если ```root == 0```, то входной вектор записывается в выходной без изменений:

```
bool TsyplakovKFromAllToOneSEQ::RunImpl() {
  auto& [data, root] = GetInput();

  if (root == 0) {
    GetOutput() = data;
  }

  return true;
}
```

### Параллельная версия алгоритма

В параллельной версии каждый процесс содержит локальный вектор данных. 
Для реализации операции ```gather``` используется древовидная схема обмена сообщениями, 
основанная исключительно на функциях ```MPI_Send``` и ```MPI_Recv```.

На начальном этапе каждый процесс:

- определяет свой номер ```(rank)``` и общее количество процессов ```(size)```;
- копирует свой локальный вектор во временный буфер;
- считает, что на текущем этапе он содержит один блок данных.

Для поддержки произвольного корневого процесса вводится относительный номер процесса:

```rel_rank = (rank - root + size) % size;```

Алгоритм выполняется итеративно с увеличением шага ```step```, принимающего значения 
степеней двойки ```(1, 2, 4, ...)```. 
На каждом шаге:

- процессы, для которых ```rel_rank % (2 * step) == 0```, 
принимают данные от соседнего процесса;
- остальные процессы передают все накопленные 
данные процессу с меньшим относительным номером и завершают участие в алгоритме.

При передаче данных процесс отправляет:

- количество передаваемых блоков;
- номера процессов-источников данных;
- соответствующие блоки данных.

После завершения всех этапов обмена весь объём данных оказывается у процесса с 
относительным номером 0, то есть у корневого процесса ```root```.

На заключительном этапе корневой процесс формирует результирующий вектор, 
размещая данные в соответствии с номерами процессов-источников, что обеспечивает 
корректный порядок элементов в выходном массиве.

Алгоритм обеспечивает логарифмическое число этапов передачи данных по 
количеству процессов и корректно работает для различных типов данных 
```(MPI_INT, MPI_FLOAT, MPI_DOUBLE)``` и произвольного выбора корневого процесса.

---

## 4. Схема распараллеливания.

Алгоритм параллельной обобщённой передачи данных от всех процессов одному (Gather).

### 1. Инициализация MPI

Все процессы участвуют в выполнении 
параллельного алгоритма и вызывают стандартные функции инициализации MPI:

- ```MPI_Comm_rank``` — для определения номера текущего процесса ```(rank)```;
- ```MPI_Comm_size``` — для определения общего количества процессов ```(size)```;

Обмен сообщениями выполняется в рамках коммуникатора ```MPI_COMM_WORLD```.

### 2. Получение входных данных

Каждый процесс независимо получает свой локальный входной вектор данных одинакового
размера, а также номер корневого процесса ```root```, 
на который необходимо собрать данные.

В отличие от задач с распределением данных, исходные данные изначально распределены между 
всеми процессами, и каждый процесс располагает только своей частью информации.

### 3. Подготовка данных к передаче

Перед началом обмена каждый процесс:

- определяет размер локального вектора ```(sendcount)```;
- определяет тип передаваемых данных ```(MPI_INT, MPI_FLOAT или MPI_DOUBLE)```;
- копирует локальный вектор во внутренний буфер для дальнейшей передачи.

Для корректной работы с произвольным корневым процессом вводится относительный номер процесса:

```rel_rank = (rank - root + size) % size```

Относительная нумерация позволяет унифицировать схему обмена и рассматривать корневой 
процесс как процесс с номером 0.

### 4. Организация передачи данных (древовидная схема)

Передача данных осуществляется по двоичному дереву процессов с использованием
только точечных операций ```MPI_Send``` и ```MPI_Recv```.

Алгоритм выполняется итеративно, при этом шаг передачи ```step``` последовательно принимает 
значения степеней двойки ```(1, 2, 4, ...)```.

На каждом шаге:

- процессы, для которых выполняется условие
```rel_rank % (2 * step) == 0```,
принимают данные от соседнего процесса с относительным номером ```rel_rank + step```;
- остальные процессы передают все накопленные данные процессу с меньшим относительным номером и завершают участие в алгоритме.

Каждый процесс передаёт:
- количество передаваемых блоков данных;
- номера процессов-источников данных;
- соответствующие блоки данных.

### 5. Агрегация данных на промежуточных узлах

Процессы, принимающие данные, объединяют полученные блоки со своими локальными данными, 
увеличивая общее количество накопленных блоков.

Таким образом, на каждом уровне дерева происходит агрегация данных от нескольких 
процессов, что позволяет сократить общее число этапов передачи до ```O(log2(size))```.

### 6. Формирование итогового результата на корневом процессе

После завершения всех этапов обмена весь объём данных оказывается у процесса с относительным номером 0, 
то есть у процесса с номером ```root```.

Корневой процесс формирует результирующий вектор, размещая данные в соответствии с 
номерами процессов-источников. Это обеспечивает корректный порядок элементов в 
выходном массиве, аналогичный стандартной операции ```MPI_Gather```.

Итоговый вектор имеет размер ```size * sendcount``` и содержит данные всех процессов.

### 7. Формирование выходных данных

После завершения алгоритма:

- только корневой процесс ```root``` сохраняет результирующий вектор в выходные данные задачи;
- остальные процессы не формируют выходных данных.

Полученный результат полностью соответствует поведению стандартной функции ```MPI_Gather```.

### 8. Особенности параллельной топологии

В реализованном алгоритме отсутствует выделенный управляющий процесс: 
все процессы изначально равноправны и участвуют в передаче данных.

Роль корневого процесса определяется параметром ```root``` и учитывается через относительную нумерацию процессов.
Древовидная схема обмена обеспечивает равномерную нагрузку и эффективную передачу данных при увеличении числа процессов.

Алгоритм не использует коллективные операции MPI и полностью основан на точечных передачах сообщений.

---

## 5. Детали реализации:

### Структура кода:

```
tsyplakov_k_vec_neighbours
    ├───common
    │   └───include
    │           common.hpp - определение типов входных/выходных/тестовых данных
    │
    ├───mpi
    │   ├───include
    │   │       ops_mpi.hpp - заголовочный файл MPI-реализации
    │   │
    │   └───src
    │           ops_mpi.cpp - MPI-реализация
    │
    ├───seq
    │   ├───include
    │   │       ops_seq.hpp - заголовочный файл SEQ-реализации
    │   │
    │   └───src
    │           ops_seq.cpp - SEQ-реализация
    │
    └───tests
        ├───functional
        │       main.cpp - функциональные тесты
        │
        └───performance
                main.cpp - тесты производительности
```

### Основные классы и функции:

```TsyplakovKFromAllToOneSEQ``` - последовательная реализация.\
```TsyplakovKFromAllToOneMPI``` - параллельная реализация.

```ValidationImpl()``` - проверка корректности входных данных перед выполнением алгоритма.\
```PreProcessingImpl()``` - подготовка данных перед основными вычислениями.\
```RunImpl()``` - основной алгоритм поиска минимальной разницы соседних элементов. PostProcessingImpl() - действия после завершения основного вычисления.

### Важные допущения и частные случаи:
При реализации параллельного алгоритма обобщённой передачи данных от всех процессов 
одному были приняты следующие допущения и учтены граничные случаи:

#### 1. Одинаковый размер локальных данных
Предполагается, что каждый процесс содержит локальный вектор одинакового размера. 
Это соответствует стандартному поведению операции ```MPI_Gather``` и упрощает формирование 
результирующего массива на корневом процессе.

#### 2. Корректность номера корневого процесса
Предполагается, что номер корневого процесса ```root``` находится в диапазоне ```[0, size - 1]```. 
Некорректное значение ```root``` приводит к неопределённому поведению и не 
рассматривается в рамках данной реализации.

#### 3. Минимальный размер данных
Алгоритм корректно работает при любом размере локального вектора, включая случай пустого 
вектора. В этом случае результирующий массив на корневом процессе также будет пустым.

#### 4. Произвольный выбор корневого процесса
Алгоритм корректно поддерживает любой выбор процесса ```root```. Для этого используется
относительная нумерация процессов, позволяющая применять единый алгоритм обмена без 
изменения структуры дерева.

#### 5. Количество процессов, не являющееся степенью двойки
Алгоритм корректно работает для любого количества процессов. На каждом этапе передачи 
проверяется существование процесса-источника, что предотвращает попытки обмена с 
несуществующими процессами.

#### 6. Синхронность обменов
Использование блокирующих операций ```MPI_Send``` и ```MPI_Recv``` требует строгого 
соблюдения порядка вызовов. Алгоритм гарантирует, что для каждого приёма сообщения 
существует соответствующая отправка, что исключает взаимные блокировки.

Сохранение порядка данных
Для восстановления корректного порядка элементов в результирующем массиве передаются 
номера процессов-источников данных. Это позволяет избежать нарушения порядка даже при 
многоэтапной агрегации данных.

### Пространственная и временная сложности алгоритмов

Последовательная версия:

В последовательной реализации алгоритма операция ```gather``` сводится к простому копированию 
локального вектора в результирующий массив, если выполняется на корневом процессе:

- Временная сложность: O(N), N - размер локального вектора данных. Копирование вектора выполняется за линейное время.
- Пространственная сложность: O(N), N - размер локального вектора. На выходе создаётся результирующий массив
- того же размера, что и локальный вектор (в последовательной версии на одном процессе).


Параллельная версия:

В параллельной реализации используется древовидная схема передачи данных, при этом каждый процесс хранит только
свои локальные данные и промежуточные буферы:

- Временная сложность: O(N/P + log2(P)), N - размер вектора, P - кол-во процессов. 
log2(P) - кол-во процессов шагов передачи по дереву.

- Пространственная сложность: O(P*N)

Таким образом, алгоритм эффективно использует память на всех процессах, а пиковое потребление приходится только 
на корневой процесс.

---

## 6. Тестовая инфраструктура

### Аппартаное обеспечение

<table border="1">
  <tr>
    <td><b>Параметр</b></td>
    <td><b>Значение</b></td>
  </tr>
  <tr>
    <td>CPU</td>
    <td>Intel Core i5-12500H (8 cores, 12 threads, 2.50 GHz, L3 Cache 18 MB)</td>
  </tr>
  <tr>
    <td>RAM</td>
    <td>16 GB, DDR4</td>
  </tr>
</table>

### Программное обеспечение 

<table border="1">
  <tr>
    <td><b>Параметр</b></td>
    <td><b>Значение</b></td>
  </tr>
  <tr>
    <td>OC</td>
    <td>Windows 10, WSL (Ubuntu 24.04)</td>
  </tr>
  <tr>
    <td>MPI</td>
    <td>OpenMPI (4.1.6)</td>
  </tr>
  <tr>
    <td>Компилятор</td>
    <td>g++ (13.3.0)</td>
  </tr>
  <tr>
    <td>Сборка</td>
    <td>Release</td>
  </tr>
</table>

### Тестовые данные 

<b>Functional tests:</b> берутся параметры теста (вектор ```data```).  

<b>Perfomance tests:</b> есть переменная ```kLocalCount```, которая отвечает за размер вектора, после чего вектор заполняется значениями.

---


### Производительность:

<b>Метрики:</b>

1. Время выполнения операции обобщённой передачи данных (gather) в миллисекундах.
2. Ускорение параллельной версии относительно последовательной.
3. Эффективность распараллеливания: (ускорение / число процессов) * 100%.


<b>Размер вектора: 7.000.000</b>
<table border="1"> <thead> <tr> <th>Режим</th> <th>Процессы</th> <th>Время (с)</th> <th>Ускорение</th> <th>Эффективность</th> </tr> </thead> <tbody> <tr> <td>SEQ</td> <td>1</td> <td>0.110</td> <td>1.00</td> <td>N/A</td> </tr> <tr> <td>MPI</td> <td>2</td> <td>0.112</td> <td>0.98</td> <td>-</td> </tr> <tr> <td>MPI</td> <td>4</td> <td>0.129</td> <td>0.85</td> <td>-</td> </tr> </tbody> </table>

### Анализ полученных результатов

Результаты показывают, что параллельная версия алгоритма не демонстрирует ускорения по сравнению с последовательной реализацией. Это объясняется спецификой поставленной задачи — обобщённой передачи данных от всех процессов одному (```MPI Gather```).
В последовательной версии алгоритма выполняется простое копирование локального вектора данных, что является линейной операцией с минимальными накладными расходами.
В параллельной же версии основное время выполнения тратится на межпроцессорный обмен большими объёмами данных. Каждый процесс отправляет свой локальный массив на корневой процесс, а использование блокирующих операций ```MPI_Send``` и ```MPI_Recv``` приводит к дополнительным затратам на синхронизацию процессов.
Несмотря на применение древовидной схемы передачи, обеспечивающей логарифмическое число этапов обмена, суммарный объём передаваемых данных увеличивается пропорционально числу процессов. 
Таким образом, параллельная реализация корректно выполняет поставленную задачу с точки зрения алгоритма, однако для данной операции приоритетными являются затраты на передачу данных, что делает последовательную версию более быстрой.

---

## 8. Выводы

В ходе работы были разработаны последовательная и параллельная версии алгоритма 
обобщённой передачи данных от всех процессов одному (операция ```gather```). Параллельная 
версия была реализована с использованием библиотеки MPI на основе точечных операций
```MPI_Send``` и ```MPI_Recv``` и протестирована при различных количествах процессов.

Результаты показали, что параллельная версия алгоритма не 
демонстрирует ускорения по сравнению с последовательной реализацией. При запуске на 
двух процессах время выполнения оказывается практически таким же как у последовательного варианта, 
а при увеличении числа процессов до четырёх наблюдается рост времени выполнения, 
несмотря на использование древовидной схемы передачи данных.

Такое поведение объясняется спецификой задачи. Операция ```gather``` не содержит 
вычислительно сложных этапов и сводится преимущественно к передаче больших объёмов 
данных между процессами. 

Дополнительные накладные расходы MPI, связанные с синхронизацией процессов, многократными
вызовами блокирующих операций ```MPI_Send``` и ```MPI_Recv```, а также необходимостью 
аккумулировать данные на корневом процессе, приводят к снижению эффективности 
распараллеливания при росте числа процессов.

Таким образом, эксперименты показывают, что для операции обобщённой передачи 
данных параллельная реализация с использованием MPI не обеспечивает выигрыша по времени 
выполнения по сравнению с последовательной версией. Последовательная реализация остаётся 
наиболее эффективной с точки зрения производительности, тогда как параллельный подход 
целесообразен в первую очередь как средство масштабируемой организации обмена данными 
в составе более сложных параллельных алгоритмов, а не как самостоятельная операция, 
предназначенная для ускорения вычислений.

---

## 9. Литература 

1. Документация по курсу: "Параллельное программирование": <https://learning-process.github.io/parallel_programming_course/ru/index.html> (Оболенский А.А, Нестеров А.Ю)
2. Список лекций по курсу "Параллельное программирование". (Сысоев А.В. ННГУ 2025 г.)
3. Список практических занятий по курсу "Параллельное программирование". (Оболенский А.А, ННГУ 2025 г.). 