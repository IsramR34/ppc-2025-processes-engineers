# Сумма элементов матрицы (SEQ | MPI)

-   **Студент** Карпич Иван Валерьевич
-   **Группа** 3823Б1ПР4
-   **Вариант** 10
-   **Технология** SEQ | MPI

---

## 1. Введение

Матрицы - одна из фундаментальных математических структур в вычислительной математике, линейной алгебре и обработке данных. Вычисление суммы элементов матрицы является базовой операцией, которая служит основой для более сложных алгоритмов. Вычисление суммы матрицы требует доступа ко всем элементам, что делает эту операцию потенциально параллелизуемой.

Цель данной работы: реализация последовательного и параллельного алгоритмов вычисления суммы элементов матрицы, анализ их производительности и масштабируемости с использованием технологии MPI.

Основные задачи:

-   Реализовать последовательный алгоритм суммирования элементов матрицы
-   Распараллелить алгоритм с использованием технологии MPI
-   Провести анализ корректности реализаций
-   Выполнить анализ производительности и масштабируемости

---

## 2. Постановка задачи

### Описание задачи

Дана матрица A размера n строк и m столбцов. Требуется вычислить сумму всех элементов матрицы.

### Входные и выходные данные

Входные данные:

-   n - количество строк матрицы (положительное целое число)
-   m - количество столбцов матрицы (положительное целое число)
-   A - массив элементов матрицы размером n × m (целые числа типа int)

Выходные данные:

-   Сумма всех элементов матрицы (целое число типа int64_t)

Типы данных:

```cpp
using InType = std::tuple<std::size_t, std::size_t, std::vector<int>>;
using OutType = std::int64_t;
```

### Ограничения

-   n > 0 и m > 0 (матрица не пуста)
-   Сумма результата не должна вызывать переполнение int64_t
-   Максимальный размер матрицы: 10000 × 10000 элементов

---

## 3. Последовательный алгоритм

### Описание алгоритма

Последовательный алгоритм вычисления суммы элементов матрицы работает по следующему принципу:

1. Инициализируется переменная для накопления суммы: `sum = 0`
2. Выполняется итерация по каждому элементу вектора (матрица представлена в виде одномерного вектора)
3. На каждой итерации текущий элемент добавляется к сумме: `sum += v`
4. После завершения всех итераций возвращается результат

### Реализация

```cpp
std::int64_t sum = 0;
for (int v : val) {
  sum += v;
}
GetOutput() = sum;
```

### Анализ сложности

-   **Временная сложность**: O(n)

### Особенности реализации

1. **Эффективность кэша**: Последовательный доступ обеспечивает хорошую локальность, что позволяет CPU эффективно использовать кэш
2. **Простота**: Использование range-based for loop (С++11) делает код понятнее и безопаснее
3. **Предсказуемость**: Высокая предсказуемость исполнения
4. **Отсутствие синхронизации**: Нет необходимости в синхронизации потоков

## 4. Параллельный алгоритм (MPI)

Параллельный алгоритм использует технологию MPI для распределения вычислений между несколькими процессами. Основная идея заключается в разделении вектора элементов матрицы между процессами, где каждый процесс вычисляет частичную сумму своей части, а затем результаты собираются с помощью операции редукции.

### Стратегия распределения данных

Вектор элементов матрицы распределяется между процессами поровну. Если всего `total_elements элементов` и `mpi_size` процессов:

-   Все процессы инициализируются с `elements_per_proc = total_elements / mpi_size` элементами
-   Остаток от деления `remainder = total_elements % mpi_size` распределяется первым процессам
-   Первые `remainder` процессов получают дополнительный элемент:

```cpp
for (int i = 0; i < remainder; ++i) {
  send_counts[i]++;
}
```

### Этапы алгоритма

#### Этап 1: Инициализация MPI и получение размера данных

```cpp
int rank = 0;
int mpi_size = 0;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);

int total_elements = 0;
if (rank == 0) {
  total_elements = static_cast<int>(val.size());
}
MPI_Bcast(&total_elements, 1, MPI_INT, 0, MPI_COMM_WORLD);
```

-   Определяется ранг (номер) текущего процесса
-   Определяется общее количество процессов
-   Процесс 0 вычисляет общее количество элементов и рассылает его всем процессам через `MPI_Bcast`

#### Этап 2: Вычисление размеров и смещений для распределения

```cpp
int elements_per_proc = total_elements / mpi_size;
int remainder = total_elements % mpi_size;

std::vector<int> send_counts(mpi_size, elements_per_proc);
std::vector<int> displacements(mpi_size, 0);

for (int i = 0; i < remainder; ++i) {
  send_counts[i]++;
}

displacements[0] = 0;
for (int i = 1; i < mpi_size; ++i) {
  displacements[i] = displacements[i - 1] + send_counts[i - 1];
}
```

-   Вычисляется количество элементов для каждого процесса (`send_counts`)
-   Первые `remainder` процессов получают на один элемент больше для балансировки нагрузки
-   Вычисляются смещения (`displacements`) для корректного распределения данных

#### Этап 3: Распределение данных между процессами

```cpp
int local_size = send_counts[rank];
std::vector<int> local_data(local_size);

MPI_Scatterv(val.data(), send_counts.data(), displacements.data(), MPI_INT,
             local_data.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);
```

-   Используется операция `MPI_Scatterv` для распределения элементов вектора
-   Каждый процесс получает свой блок элементов в `local_data`
-   `send_counts` определяет, сколько элементов получает каждый процесс
-   `displacements` определяет, откуда начинать отправку для каждого процесса

#### Этап 4: Локальное вычисление суммы

```cpp
std::int64_t local_sum = std::accumulate(local_data.begin(), local_data.end(),
                                         static_cast<std::int64_t>(0));
```

-   Каждый процесс независимо вычисляет сумму своих элементов
-   Используется функция `std::accumulate` из стандартной библиотеки C++
-   Начальное значение аккумулятора: 0 (типа `int64_t`)

#### Этап 5: Сбор результатов (редукция)

```cpp
std::int64_t global_sum = 0;
MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT64_T, MPI_SUM, 0, MPI_COMM_WORLD);
```

-   Используется операция `MPI_Reduce` для сбора всех локальных сумм
-   Все процессы отправляют свою локальную сумму
-   Процесс с рангом 0 получает глобальную сумму (сумму всех локальных сумм)
-   Используется операция `MPI_SUM` для суммирования всех значений

#### Этап 6: Рассылка результата всем процессам

```cpp
MPI_Bcast(&global_sum, 1, MPI_INT64_T, 0, MPI_COMM_WORLD);
```

-   Результат распределяется всем процессам с помощью `MPI_Bcast`
-   Процесс 0 отправляет глобальную сумму остальным процессам
-   Все процессы получают одинаковый результат

### Особенности реализации

1. **Балансировка нагрузки**: Остаток от деления распределяется среди первых процессов, обеспечивая более равномерное разделение нагрузки
2. **Использование MPI_Scatterv**: Позволяет распределять данные переменного размера между процессами
3. **Использование std::accumulate**: Безопасный и удобный способ вычисления суммы элементов вектора
4. **Эффективная редукция**: Операция MPI_Reduce использует древовидную архитектуру для минимизации количества шагов communication
5. **Двойная синхронизация**: `MPI_Bcast` вначале синхронизирует размер данных, а `MPI_Reduce` + `MPI_Bcast` в конце собирают и распространяют результат

---

## 5. Детали реализации

### Структура проекта

| Файл                         | Назначение                                    |
| ---------------------------- | --------------------------------------------- |
| `common/include/common.hpp`  | Определение входных и выходных типов данных   |
| `seq/src/ops_seq.hpp`        | Заголовочный файл последовательной реализации |
| `seq/src/ops_seq.cpp`        | Реализация последовательного алгоритма        |
| `mpi/src/ops_mpi.hpp`        | Заголовочный файл MPI реализации              |
| `mpi/src/ops_mpi.cpp`        | Реализация параллельного алгоритма            |
| `tests/functional/main.cpp`  | Функциональные тесты                          |
| `tests/performance/main.cpp` | Тесты производительности                      |

---

## 6. Экспериментальная среда

| Компонент  | Значение                                |
| ---------- | --------------------------------------- |
| CPU        | Apple M2 (8 cores)                      |
| RAM        | 16 GB                                   |
| ОС         | OS: Ubuntu 24.04 (DevContainer / Mac)   |
| Компилятор | GCC 13.3.0 (g++), C++20, CMake, Release |
| MPI        | mpirun (Open MPI) 4.1.6                 |

Тестовые данные:
В функциональных тестах использовались вручную подготовленные текстовые файлы из каталога `data/`, содержащие размеры матрицы и её элементы. Ожидаемые результаты закодированы в параметрах тестов. Для оценки производительности матрица генерируется автоматически в коде (квадратная 10000×10000, фиксированного размера).

Генерация данных происходила при помощи стандартной библиотеки C++ (std::mt19937).

---

## 7. Анализ

### 7.1 Корректность

Корректность работы проверялась следующими способами:

**Функциональные тесты:**

-   Использовались 7 вручную подготовленных тестовых наборов с известными эталонными результатами
-   Матрицы различных размеров (от малых до средних)
-   Для каждого набора ожидаемый результат закодирован в параметрах теста
-   Оба варианта (SEQ и MPI) проверяют выходное значение против эталона

**Проверка инвариантов:**

-   Валидация входных данных: проверяется, что размеры матрицы положительны
-   Проверяется, что результат суммирования корректен для различных размеров матриц
-   Проверяется консистентность результатов между SEQ и MPI реализациями

**Результат:**
Все функциональные тесты проходят успешно для обеих реализаций, подтверждая корректность алгоритмов.

### 7.2 Производительность

Производительность измерялась на матрице размером 10000 × 10000 элементов при различном количестве MPI процессов.

### 7.3 Результаты производительности

По результатам выполнения алгоритмов на тестовых данных: матрица int 10000х10000 элементов, сгенерированных случайным образом
я замерил следующие характеристики:
| Режим | Количество процессов | Время, с | Ускорение | Эффективность |
|-------|----------------------|--------------|------------|---------------|
| SEQ | 1 | 0.0320909023 | 1.00 | N/A |
| MPI | 2 | 0.0156074334 | 2.06 | 1.03% |
| MPI | 4 | 0.0087096500 | 3.57 | 0.89% |

---

## 8. Выводы

В ходе выполнения работы были реализованы последовательный и параллельный (MPI) алгоритмы вычисления суммы элементов матрицы.

### 8.1 Основные результаты:

-   ✓ Реализована последовательная версия алгоритма
-   ✓ Реализована параллельная версия с использованием технологии MPI
-   ✓ Функциональные тесты (7 наборов) покрывают корректность работы
-   ✓ Проведён сравнительный анализ производительности

---

## 9. Источники

1. Сысоев А. В. Курс лекций по параллельному программированию
2. Документация Open MPI https://www.open-mpi.org/doc/
3. Microsoft Функции MPI https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions

---

## 10. Приложение

### A. Фрагмент кода: Последовательная реализация

```cpp
std::int64_t sum = 0;
for (int v :  val) {
  sum += v;
}
GetOutput() = sum;
```

### B. Фрагмент кода: Распределение и вычисление (MPI)

```cpp
// Вычисление размеров и смещений
int elements_per_proc = total_elements / mpi_size;
int remainder = total_elements % mpi_size;

std::vector<int> send_counts(mpi_size, elements_per_proc);
std::vector<int> displacements(mpi_size, 0);

for (int i = 0; i < remainder; ++i) {
  send_counts[i]++;
}

displacements[0] = 0;
for (int i = 1; i < mpi_size; ++i) {
  displacements[i] = displacements[i - 1] + send_counts[i - 1];
}

// Распределение данных
MPI_Scatterv(val.data(), send_counts.data(), displacements.data(),
             MPI_INT, local_data.data(), local_size, MPI_INT, 0,
             MPI_COMM_WORLD);

// Локальное вычисление
std::int64_t local_sum = std::accumulate(local_data.begin(), local_data.end(),
                                         static_cast<std::int64_t>(0));

// Редукция
std::int64_t global_sum = 0;
MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT64_T, MPI_SUM, 0,
           MPI_COMM_WORLD);

// Рассылка результата
MPI_Bcast(&global_sum, 1, MPI_INT64_T, 0, MPI_COMM_WORLD);
```
