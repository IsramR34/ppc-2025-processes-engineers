# Линейная фильтрация изображений (вертикальное разбиение). Ядро Гаусса 3x3

- Студент: Марьин Лев Михайлович 
- Группа: 3823Б1ПР3
- Технология: MPI|SEQ
- Вариант: 27

## 1. Введение

Линейная фильтрация изображений является одной из базовых операций в обработке изображений. Фильтр Гаусса используется для сглаживания изображений, уменьшения шума и подготовки данных для дальнейшей обработки. При работе с большими изображениями последовательная обработка может занимать значительное время, поэтому распараллеливание алгоритма с использованием технологии MPI позволяет существенно ускорить вычисления.

Целью данной работы является реализация параллельного алгоритма линейной фильтрации изображений с ядром Гаусса 3x3 с использованием вертикального разбиения данных между процессами MPI.

## 2. Постановка задачи

Задача заключается в применении фильтра Гаусса 3x3 к изображению, представленному в виде одномерного массива пикселей. Изображение может быть цветным или в оттенках серого.

**Входные данные:**
- Одномерный массив пикселей изображения
- Ширина и высота изображения

**Выходные данные:**
- Одномерный массив пикселей отфильтрованного изображения

**Ядро Гаусса 3x3:**
```
1/16 * | 1  2  1 |
       | 2  4  2 |
       | 1  2  1 |
```

**Ограничения:**
- Для граничных пикселей используется нулевой padding (значение 0 за пределами изображения)
- Результат должен быть идентичен последовательной версии алгоритма

## 3. Базовый алгоритм (последовательный)

Последовательный алгоритм применяет ядро Гаусса к каждому пикселю изображения:

1. Для каждого пикселя (x, y) в изображении:
   - Вычисляется взвешенная сумма значений пикселей в окрестности 3x3
   - Для пикселей за границами изображения используется значение 0
   - Результат делится на сумму весов ядра (16) и ограничивается диапазоном [0, 255]

2. Результат записывается в выходной массив

**Сложность:** O(W * H), где W - ширина, H - высота изображения

**Псевдокод:**
```
для каждого row от 0 до height-1:
    для каждого col от 0 до width-1:
        sum = 0
        для каждого ky от -1 до 1:
            для каждого kx от -1 до 1:
                ny = row + ky
                nx = col + kx
                если (ny >= 0 и ny < height и nx >= 0 и nx < width):
                    pixel_value = input[ny * width + nx]
                иначе:
                    pixel_value = 0
                sum += pixel_value * kernel[ky+1][kx+1]
        output[row * width + col] = clamp(sum / 16, 0, 255)
```

## 4. Схема распараллеливания

Для распараллеливания используется вертикальное разбиение изображения: изображение делится на вертикальные полосы (столбцы), каждая из которых обрабатывается отдельным процессом MPI.

**Распределение данных:**
- Процесс с rank 0 получает исходное изображение
- Изображение разбивается на N частей по столбцам, где N - количество процессов
- Каждый процесс получает свою часть столбцов плюс по одному граничному столбцу с каждой стороны для корректного вычисления свёртки

**Коммуникационная схема:**
1. Процесс 0 рассылает размеры изображения (width, height) всем процессам через MPI_Bcast
2. Процесс 0 вычисляет распределение столбцов между процессами
3. Процесс 0 отправляет каждому процессу его часть данных через MPI_Send (включая граничные столбцы)
4. Каждый процесс применяет фильтр Гаусса к своей части
5. Процессы отправляют результаты обратно процессу 0 через MPI_Send
6. Процесс 0 собирает результаты и рассылает финальный результат всем процессам через MPI_Bcast

**Особенности:**
- При количестве процессов больше количества столбцов, некоторые процессы не получают данных (local_col_count == 0)
- Граничные столбцы (padding) необходимы для корректного вычисления свёртки на границах разбиения
- Все процессы должны иметь финальный результат для корректной проверки в тестах

## 5. Детали реализации

**Структура кода:**
- `common/include/common.hpp` - определение типов данных (ImageData, InType, OutType)
- `seq/src/ops_seq.cpp` - последовательная реализация
- `mpi/src/ops_mpi.cpp` - параллельная реализация с MPI
- `tests/functional/main.cpp` - функциональные тесты
- `tests/performance/main.cpp` - тесты производительности

**Ключевые функции MPI реализации:**
- `ComputeColDistribution()` - вычисление распределения столбцов между процессами
- `ComputePadding()` - вычисление необходимых граничных столбцов
- `ExtractLocalData()` - извлечение локальной части данных из исходного изображения
- `SendDataToWorkers()` - отправка данных рабочим процессам
- `ApplyFilterToLocalData()` - применение фильтра к локальным данным
- `ReceiveResultsFromWorkers()` - сбор результатов от рабочих процессов

**Обработка граничных случаев:**
- Изображения размером 1x1 обрабатываются корректно
- Когда процессов больше чем столбцов, лишние процессы пропускаются
- Граничные пиксели обрабатываются с нулевым padding

**Использование памяти:**
- Каждый процесс хранит только свою часть изображения плюс граничные столбцы
- Общий объем памяти: O(W * H / P + H), где P - количество процессов

## 6. Экспериментальная установка

**Аппаратное обеспечение**
- Процессор: Intel Core i5-12450H
- ОЗУ: 16 ГБ
- ОС: Windows 10 Pro

**Программное обеспечение**
- Компилятор: MSVC 19.36
- MPI: MS-MPI 10.0
- CMake: 4.1.2
- Фреймворк тестирования: Google Test

**Переменные окружения:**
- Количество процессов MPI (1, 2, 4, 6, 8)
- Другие переменные определяются тестовым окружением

**Тестовые данные:**
- Размер изображения: 3000x3000 пикселей
- Данные генерируются детерминированным образом на основе размера изображения
- Файл: `tests/performance/main.cpp`

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность реализации проверялась следующими способами:

1. **Функциональные тесты:** Реализовано 12 тестовых случаев, покрывающих различные сценарии:
   - Однородные изображения
   - Градиентные изображения
   - Шахматный узор
   - Вертикальные и горизонтальные полосы
   - Случайные данные
   - Граничные случаи (1x1, 3x3, строка, столбец)
   - Большие изображения (100x100, 50x50)

2. **Сравнение с эталоном:** Результаты параллельной реализации сравниваются с результатами последовательной версии алгоритма, применяемой к тем же входным данным.

3. **Покрытие тестами:** Все тесты проходят успешно как для последовательной, так и для параллельной версии, что подтверждает идентичность результатов.

### 7.2 Производительность

Результаты измерений времени выполнения, ускорения и эффективности для режимов task_run и task_pipeline представлены в таблицах ниже.

**Таблица 1: Результаты для режима task_run**

| Режим | Процессов | Время, с | Ускорение | Эффективность |
|-------|-----------|----------|-----------|---------------|
| seq   | 1         | 1.066    | 1.00      | N/A           |
| mpi   | 1         | 1.180    | 0.90      | 90.4%         |
| mpi   | 2         | 0.759    | 1.40      | 70.2%         |
| mpi   | 4         | 0.555    | 1.92      | 48.0%         |
| mpi   | 6         | 0.619    | 1.72      | 28.7%         |
| mpi   | 8         | 0.617    | 1.73      | 21.6%         |

**Таблица 2: Результаты для режима task_pipeline**

| Режим | Процессов | Время, с | Ускорение | Эффективность |
|-------|-----------|----------|-----------|---------------|
| seq   | 1         | 1.146    | 1.00      | N/A           |
| mpi   | 1         | 1.323    | 0.87      | 86.6%         |
| mpi   | 2         | 0.875    | 1.31      | 65.5%         |
| mpi   | 4         | 0.568    | 2.02      | 50.5%         |
| mpi   | 6         | 0.623    | 1.84      | 30.7%         |
| mpi   | 8         | 0.630    | 1.82      | 22.7%         |

**Расчёт метрик**:
- Ускорение = T_seq / T_mpi
- Эффективность = (Speedup / P) × 100%

**Анализ результатов:**

1. **Ускорение:** Параллельная версия демонстрирует ускорение до 2.02 раз на 4 процессах для режима task_pipeline и до 1.92 раз для режима task_run. Однако при дальнейшем увеличении количества процессов (6 и 8) ускорение практически не растет и даже немного снижается, что указывает на насыщение параллелизма.

2. **Эффективность:** Эффективность параллелизации снижается с ростом количества процессов. На 2 процессах эффективность составляет 70.2% для task_run и 65.5% для task_pipeline. На 4 процессах эффективность падает до 48.0% и 50.5% соответственно. При 6 и 8 процессах эффективность значительно снижается (до 21.6-30.7%), что указывает на преобладание накладных расходов на коммуникацию над выигрышем от параллелизации.

3. **Сравнение режимов:** Режим task_run показывает немного лучшую эффективность по сравнению с task_pipeline на малом количестве процессов, но разница незначительна.

4. **Оптимальное количество процессов:** На основе результатов видно, что оптимальное количество процессов для данного размера задачи (3000x3000) составляет 4 процесса, где достигается максимальное ускорение при приемлемой эффективности (около 50%).

4. **Узкие места:**
   - Коммуникационные задержки при передаче данных между процессами становятся доминирующими при большом количестве процессов
   - Неравномерное распределение столбцов при делении ширины на количество процессов
   - Накладные расходы на синхронизацию процессов (MPI_Bcast, MPI_Send/MPI_Recv)
   - Для данного размера задачи (3000x3000) объем вычислений на процесс становится недостаточным при большом количестве процессов

5. **Масштабируемость:** Алгоритм демонстрирует хорошую масштабируемость до 4 процессов, где достигается максимальное ускорение. При дальнейшем увеличении количества процессов (6, 8) эффективность значительно падает из-за преобладания коммуникационных накладных расходов над объемом вычислений. Для больших изображений масштабируемость может быть лучше.

## 8. Выводы

В ходе выполнения работы была успешно реализована параллельная версия алгоритма линейной фильтрации изображений с ядром Гаусса 3x3 с использованием технологии MPI и вертикального разбиения данных.

**Основные достижения:**
- Реализована корректная параллельная версия алгоритма, дающая идентичные результаты с последовательной версией
- Достигнуто ускорение до 2.02 раз на 4 процессах для режима task_pipeline
- Обеспечено покрытие тестами более 90% кода
- Все функциональные и производительные тесты проходят успешно

**Ограничения:**
- Эффективность значительно снижается с ростом количества процессов из-за коммуникационных накладных расходов
- При большом количестве процессов (больше ширины изображения) некоторые процессы остаются неиспользованными
- Для изображения размером 3000x3000 оптимальное количество процессов составляет 4, дальнейшее увеличение не дает существенного выигрыша
- Алгоритм эффективен для больших изображений, для маленьких накладные расходы на коммуникацию могут превысить выигрыш от параллелизации

**Возможные улучшения:**
- Использование MPI_Scatterv для более эффективного распределения данных
- Динамическая балансировка нагрузки между процессами

## 9. Список источников информации

1. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.
2. Преснухин Л. Н. Параллельное программирование с использованием стандарта OpenMP. — Москва: Издательство МГУ, 2010.
2. Parallel Programming Course Slides - https://learning-process.github.io/parallel_programming_slides/
3. Microsoft. Microsoft MPI Documentation - https://learn.microsoft.com/en-us/message-passing-interface/microsoft-mpi 

## Приложение

Пример ключевого фрагмента кода распределения данных:

```cpp
void ComputeColDistribution(int width, int size, 
                            std::vector<int> &col_counts, 
                            std::vector<int> &col_starts) {
  int cols_per_proc = width / size;
  int extra_cols = width % size;
  int current_col = 0;
  for (int proc = 0; proc < size; ++proc) {
    col_starts[proc] = current_col;
    col_counts[proc] = cols_per_proc + ((proc < extra_cols) ? 1 : 0);
    current_col += col_counts[proc];
  }
}
```

Пример применения фильтра к локальным данным:

```cpp
uint8_t ApplyKernelLocal(const std::vector<uint8_t> &local_input, 
                         int ext_col_count, int height, 
                         int row, int lx, int left_pad) {
  int sum = 0;
  for (int ky = -1; ky <= 1; ++ky) {
    for (int kx = -1; kx <= 1; ++kx) {
      int ny = row + ky;
      int local_nx = lx + left_pad + kx;
      uint8_t pixel_value = 0;
      if (ny >= 0 && ny < height && 
          local_nx >= 0 && local_nx < ext_col_count) {
        pixel_value = local_input[(ny * ext_col_count) + local_nx];
      }
      sum += pixel_value * kGaussKernel.at(ky + 1).at(kx + 1);
    }
  }
  return static_cast<uint8_t>(std::clamp(sum / kKernelSum, 0, 255));
}
```

