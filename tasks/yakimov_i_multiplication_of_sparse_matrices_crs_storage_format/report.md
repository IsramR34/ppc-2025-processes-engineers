# Нахождение максимальных значений по строкам матрицы Якимов Илья

- Student: Якимов Илья Владимирович, group 3823Б1ПР2
- Technology: SEQ | MPI 
- Variant: 4

## 1. Introduction
Разработка однопоточного алгоритма (SEQ) умножения разреженных матриц в формате хранения CRS (Compressed Row Storage) и его параллельной реализации (MPI). Цель - ускорение обработки больших разреженных матриц за счет распределения вычислений между несколькими процессами с минимизацией коммуникационных затрат.

## 2. Problem Statement
Для заданных двух разреженных матриц A (M×K) и B (K×N) в формате CRS вычислить произведение C = A × B, затем найти сумму всех ненулевых элементов результирующей матрицы C.

Вход: два текстовых файла с разреженными матрицами в формате CRS (целые размеры, затем построчно: количество ненулевых элементов, индексы столбцов, значения)

Выход: сумма всех ненулевых элементов результирующей матрицы

Ограничения: время работы <1 секунды в functional тестах

## 3. Baseline Algorithm (Sequential)

'''for (int i = 0; i < A.rows; ++i) {
    std::fill(row_values.begin(), row_values.end(), 0.0);
    
    int row_start_a = A.row_pointers[i];
    int row_end_A = A.row_pointers[i + 1];
    
    for (int k = row_start_a; k < row_end_A; ++k) {
        int col_A = A.col_indices[k];
        double val_A = A.values[k];
        
        int row_start_B = B.row_pointers[col_A];
        int row_end_B = B.row_pointers[col_A + 1];
        
        for (int l = row_start_B; l < row_end_B; ++l) {
            int col_B = B.col_indices[l];
            double val_B = B.values[l];
            row_values[col_B] += val_A * val_B;
        }
    }
    
    // Сохранение ненулевых элементов строки i в матрицу C
}'''

## 4. Parallelization Scheme

### Распределение данных: 

Блочное распределение строк матрицы A между процессами. Каждый процесс получает свой набор строк матрицы A для умножения на всю матрицу B.

### Коммуникация:

Процесс 0 читает обе матрицы из файлов
Матрица A распределяется по строкам между всеми процессами
Матрица B широковещательно рассылается всем процессам (так как нужна полностью каждому процессу)
Каждый процесс вычисляет произведение своих строк A на матрицу B
Результаты (вычисленные строки матрицы C) собираются на процесс 0

### Роли рангов:

Rank 0: мастер-процесс (чтение, распределение, сбор результатов)
Rank 1..N-1: рабочие процессы (обработка своих блоков строк)

## 5. Implementation Details
### Структура кода:
Используем команду "tree tasks/yakimov_i_max_values_in_matrix_rows/" для того чтобы узнать структуру проекта:

tasks/yakimov_i_multiplication_of_sparse_matrices_crs_storage_format/
.
├── common
│   └── include
│       └── common.hpp
├── data
│   ├── A_1.txt
│   ├── A_2.txt
│   ├── A_27.txt
│   ├── A_28.txt
│   ├── A_29.txt
│   ├── A_3.txt
│   ├── A_30.txt
│   ├── A_31.txt
│   ├── A_32.txt
│   ├── A_33.txt
│   ├── A_34.txt
│   ├── A_35.txt
│   ├── A_36.txt
│   ├── A_4.txt
│   ├── A_5.txt
│   ├── B_1.txt
│   ├── B_2.txt
│   ├── B_27.txt
│   ├── B_28.txt
│   ├── B_29.txt
│   ├── B_3.txt
│   ├── B_30.txt
│   ├── B_31.txt
│   ├── B_32.txt
│   ├── B_33.txt
│   ├── B_34.txt
│   ├── B_35.txt
│   ├── B_36.txt
│   ├── B_4.txt
│   └── B_5.txt
├── generate_sparse_matrices.py
├── info.json
├── mpi
│   ├── include
│   │   └── ops_mpi.hpp
│   └── src
│       └── ops_mpi.cpp
├── report.md
├── seq
│   ├── include
│   │   └── ops_seq.hpp
│   └── src
│       └── ops_seq.cpp
├── settings.json
└── tests
    ├── functional
    │   └── main.cpp
    └── performance
        └── main.cpp

13 directories, 41 files

### Ключевые методы (вспомогательные для RunImpl(), с целью декомпозиции функции и избежания ее перегрузки):
- BroadcastMatrixInfo() - передача размеров матриц всем процессам
- DistributeMatrixA() - распределение строк матрицы A по процессам
- DistributeMatrixB() - широковещательная рассылка матрицы B
- MultiplyLocalRows() - умножение локальных строк на матрицу B
- GatherResults() - сбор результатов на процесс 0

### Особенности:
- Использование формата CRS для эффективного хранения разреженных матриц
- Минимизация коммуникаций: матрица B отправляется один раз всем процессам
- Обработка неравномерного распределения строк при делении M на N процессов
- Эффективный алгоритм умножения с учетом разреженности
- Проверка совместимости размеров матриц (cols_A == rows_B)

## 6. Experimental Setup
- Hardware/OS: CPU model: Intel Core i7 12700H, cores/threads: 14/20, RAM: 16GB, OS version: Ubuntu 24.04.3 LTS
- Toolchain: compiler: gcc, version: 14, MPI: OpenMPI 4.1.2, build type (Release/RelWithDebInfo): Release
- Environment: PPC_NUM_THREADS / PPC_NUM_PROC: 1 / 8, other relevant vars: -
- Data: разреженные матрицы размером от 100×100 до 5000×5000 с плотностью ~1-5% 

## 7. Results and Discussion

### 7.1 Correctness
- Проверка против последовательной реализации (SEQ vs MPI)
- Unit-тесты с эталонными значениями для краевых случаев
- Тестирование на матрицах разного размера и плотности
- Валидация результатов умножения (C = A × B)
- Проверка обработки пустых строк и полностью нулевых матриц

### 7.2 Performance
Результаты в столбце "Time" представлены как среднее между 3 запусками performance тестов

Speedup = T_seq / T_parallel
Efficiency = Speedup / Count * 100%

#### Измерения "чистого" времени вычислений максимальных элементов по строкам матрицы - task_run

| Mode        | Count |    Time, ms    | Speedup | Efficiency |
|-------------|-------|----------------|---------|------------|
| seq         | 1     | 0.0413610458   | 1.00    | N/A        |
| mpi         | 4     | 0.0218625654   | 1.89    | 47.3%      |
| mpi         | 8     | 0.0235665934   | 1.76    | 22.0%      |
| mpi         | 12    | 0.0419393934   | 0.99    | 8.2%       |
| mpi         | 20    | 0.0816727566   | 0.51    | 2.5%       |

#### Измерения полного времени вычислений ("чистое" + затраты на открытие файла, считывание и коммуникацию процессов) - pipeline

| Mode        | Count |    Time, ms    | Speedup | Efficiency |
|-------------|-------|----------------|---------|------------|
| seq         | 1     | 0.2071322441   | 1.00    | N/A        |
| mpi         | 4     | 0.1335902202   | 1.55    | 38.8%      |
| mpi         | 8     | 0.1587244536   | 1.31    | 16.4%      |
| mpi         | 12    | 0.2072567540   | 1.00    | 8.3%       |
| mpi         | 20    | 0.3158101470   | 0.66    | 3.3%       |

Анализ результатов:

Вычислительная эффективность (task_run):

- На 4 процессах: Speedup = 1.89, Efficiency = 47.3%
- На 8 процессах: Speedup = 1.76, Efficiency = 22.0%
- На 12 процессах: практически нет ускорения (Speedup = 0.99)
- На 20 процессах: значительное замедление (Speedup = 0.51)

Эффективность полного процесса (pipeline):

- На 4 процессах: Speedup = 1.55, Efficiency = 38.8%
- На 8 процессах: Speedup = 1.31, Efficiency = 16.4%
- На 12 процессах: нет ускорения (Speedup = 1.00)
- На 20 процессах: замедление (Speedup = 0.66)

Критический анализ:

- Сверхлинейное ускорение отсутствует даже для "чистых" вычислений
- Эффективность падает значительно быстрее с ростом числа процессов
- Оптимальное число процессов: 4 (максимальная эффективность 47.3% для вычислений)
- Сверх 8 процессов нецелесообразно - эффективность ниже 25%

Ограничения масштабируемости:

- Недостаточный объем вычислений: Для матриц размером, дающим время выполнения ~0.04 мс, накладные расходы MPI превосходят вычислительную работу
- Критический дисбаланс: При 12+ процессах каждый процесс получает слишком мало строк для обработки
- Доминирование коммуникаций: Время передачи данных превышает время вычислений уже при 8 процессах
- Синхронизационные издержки: Барьеры и коллективные операции съедают преимущество параллелизма

"Бутылочные горлышки" (уточненные):

- Стартовые затраты MPI: Инициализация MPI среды для коротких задач
- Микросекундные вычисления: Алгоритм слишком быстрый для параллелизации на MPI
- Сбор мелких фрагментов: Сбор результатов с многих процессов для малого объема данных

Порог эффективности:
- Алгоритм эффективен для матриц размером от 1000×1000 элементов. Для меньших матриц последовательная версия может оказаться быстрее из-за накладных расходов параллелизации.

## 8. Conclusions
- MPI реализация обеспечивает значительное ускорение
- Наилучшая эффективность достигается при умеренном числе процессов (от 4 до 8 процессов, так как дальше идут неоправданные затраты времени на коммуникацию рабочих процессов с мастер процессом)
- Алгоритм хорошо масштабируется для больших матриц
- Основное ограничение - коммуникационные затраты

## 9. References 
1. OpenMPI документация: <https://www.open-mpi.org/>
2. MPI стандарт: <https://www.mpi-forum.org/>
3. Курс параллельного программирования: материалы курса <https://learning-process.github.io/parallel_programming_course/ru/common_information/report.html>
4. Мастер проекта: <https://github.com/learning-process/ppc-2025-processes-engineers>