# Интегрирование - метод трапеций

- Студент: Точилин Евгений Дмитриевич 
- Группа: 3823Б1ПР3
- Технология: MPI
- Вариант: 20

## 1. Введение

Численное интегрирование - одна из основных задач вычислительной математики. Метод трапеций является простым и эффективным способом приближенного вычисления определенного интеграла функции. При работе с большими объемами данных или высокой точностью вычислений последовательный алгоритм может быть слишком медленным. Параллелизация с использованием технологии MPI позволяет распределить вычисления между несколькими процессами, что значительно ускоряет выполнение задачи.

Целью данной работы - реализация последовательного и параллельного алгоритмов численного интегрирования методом трапеций с использованием MPI, а также анализ их производительности и корректности.

## 2. Постановка задачи

Задача заключается в вычислении определенного интеграла функции f(x) на отрезке [a, b] методом трапеций.

Формальная постановка задачи:
- Входные данные: нижняя граница интегрирования a, верхняя граница b, количество интервалов разбиения n, функция f(x)
- Выходные данные: приближенное значение интеграла I

Формула метода трапеций:
I ≈ h * (f(a)/2 + f(x1) + f(x2) + ... + f(x_{n-1}) + f(b)/2)

где h = (b - a) / n - шаг разбиения, xi = a + i * h - точки разбиения.

Ограничения:
- Количество интервалов n должно быть положительным целым числом
- Нижняя граница a должна быть меньше верхней границы b
- Функция f(x) должна быть определена на всем отрезке [a, b]

## 3. Базовый алгоритм 

Последовательный алгоритм метода трапеций реализуется следующим образом:

1. Валидация входных данных: проверка корректности границ интегрирования, количества интервалов и наличия функции.

2. Препроцессинг: сохранение входных параметров в локальные переменные для удобства работы.

3. Основной алгоритм:
   - Вычисление шага разбиения: h = (b - a) / n
   - Инициализация суммы: sum = (f(a) + f(b)) / 2.0
   - Итерация по всем промежуточным точкам от 1 до n-1:
     - Вычисление координаты точки: x = a + i * h
     - Добавление значения функции в сумму: sum += f(x)
   - Вычисление результата: result = h * sum

4. Постпроцессинг: сохранение результата в выходную переменную.

Алгоритм имеет временную сложность O(n), где n - количество интервалов разбиения. Каждая итерация требует одного вызова функции f(x), что делает алгоритм эффективным для функций, вычисление которых занимает постоянное время.

## 4. Схема распараллеливания

Параллелизация алгоритма методом трапеций с использованием MPI основана на распределении интервалов разбиения между процессами.

Схема распараллеливания:

1. Распределение данных:
   - Процесс с рангом 0 (главный процесс) получает все входные параметры
   - Главный процесс рассылает параметры интегрирования (границы a, b и количество интервалов n) всем остальным процессам с помощью MPI_Bcast

2. Распределение работы:
   - Общее количество интервалов n делится между процессами
   - Базовое количество интервалов на процесс: base_intervals = n / size
   - Остаток распределяется между первыми процессами: remainder = n % size
   - Каждый процесс получает свой диапазон интервалов для вычисления

3. Коммуникационная схема:
   - Главный процесс (rank 0) вычисляет диапазоны для всех процессов
   - Для каждого процесса от 1 до size-1 главный процесс отправляет через MPI_Send:
     - Начальный индекс интервала (local_start)
     - Количество интервалов для обработки (local_count)
     - Шаг разбиения (step)
   - Остальные процессы получают свои параметры через MPI_Recv

4. Параллельные вычисления:
   - Каждый процесс вычисляет частичную сумму для своего диапазона интервалов
   - Для каждого интервала вычисляется площадь трапеции: ((f(x_left) + f(x_right)) / 2.0) * step

5. Сбор результатов:
   - Все процессы отправляют свои частичные суммы главному процессу через MPI_Reduce с операцией MPI_SUM
   - Главный процесс получает итоговую сумму всех частичных результатов
   - Результат рассылается всем процессам через MPI_Bcast для обеспечения согласованности

Топология коммуникаций представляет собой звезду с главным процессом в центре, который координирует распределение работы и сбор результатов.

## 5. Детали реализации

Структура кода:

1. Общие определения (common/include/common.hpp):
   - Структура IntegrationInput для входных данных
   - Типы InType, OutType, BaseTask

2. Последовательная реализация:
   - Заголовочный файл: seq/include/ops_seq.hpp
   - Реализация: seq/src/ops_seq.cpp
   - Класс TochilinEIntegrationTrapezoidSEQ наследуется от BaseTask

3. Параллельная реализация:
   - Заголовочный файл: mpi/include/ops_mpi.hpp
   - Реализация: mpi/src/ops_mpi.cpp
   - Класс TochilinEIntegrationTrapezoidMPI наследуется от BaseTask

Ключевые особенности реализации:

- Валидация выполняется только на главном процессе для экономии ресурсов
- Использование std::array вместо C-style массивов для соответствия современным стандартам C++
- Равномерное распределение нагрузки с учетом остатка от деления
- Синхронизация результатов через MPI_Bcast в PostProcessingImpl для обеспечения корректной работы тестов

Важные предположения:

- Функция f(x) должна быть определена и вычислима на всем отрезке [a, b]
- Количество процессов не превышает количество интервалов разбиения
- Все процессы имеют доступ к одной и той же функции f(x)

Учет граничных случаев:

- Обработка случая, когда количество интервалов не делится нацело на количество процессов
- Корректная работа при одном процессе (последовательный режим)
- Проверка валидности входных данных перед началом вычислений

Использование памяти:

- Каждый процесс хранит только свои локальные переменные и частичную сумму
- Общий объем памяти O(1) на процесс, не зависит от количества интервалов
- Коммуникационные буферы имеют фиксированный размер (3 элемента типа double)

## 6. Экспериментальная установка

Окружение для тестирования:

- Процессор: AMD Ryzen 5 7500F
- Количество ядер: 6 (физических ядер) (3.70 GHz)
- ОЗУ: 32GB
- ОС: Windows 11 64-bit

Инструментарий:

- Компилятор: MSVC (Microsoft Visual C++) с поддержкой C++17
- Тип сборки: Release
- Версия MPI: OpenMPI 3.1
- Система сборки: CMake 4.1.3

Переменные окружения:

- PPC_NUM_PROC: количество процессов MPI (1, 2, 4, 6, 8)
- Другие релевантные переменные окружения будут указаны при необходимости

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность реализации проверялась следующими способами:

1. Функциональные тесты:
   - Тестирование на различных функциях: линейная, квадратичная, кубическая, sin, cos, exp, sqrt
   - Проверка граничных случаев: минимальное количество интервалов, малые и большие интервалы, отрицательные границы
   - Валидация входных данных: некорректные параметры, null-функции

2. Сравнение результатов:
   - Результаты параллельного алгоритма сравниваются с результатами последовательного
   - Допустимая погрешность: 1e-10 для большинства тестов
   - Все тесты проходят успешно на различных количествах процессов

3. Проверка согласованности:
   - Результаты на всех процессах идентичны благодаря использованию MPI_Bcast
   - Корректность распределения нагрузки проверяется через сравнение с эталонными значениями

Всего реализовано 31 функциональный тест, которые покрывают различные сценарии использования и граничные случаи.

### 7.2 Производительность

Результаты производительности для режима task_run:

| Процессов | Время, с | Ускорение | Эффективность |
|-----------|----------|-----------|---------------|
| 1         | 0.461    | 1.00      | N/A           |
| 2         | 0.289    | 1.59      | 79.7%         |
| 4         | 0.195    | 2.37      | 59.3%         |
| 6         | 0.185    | 2.49      | 41.5%         |
| 8         | 0.186    | 2.48      | 30.9%         |

Результаты производительности для режима task_pipeline:

| Процессов | Время, с | Ускорение | Эффективность |
|-----------|----------|-----------|---------------|
| 1         | 0.483    | 1.00      | N/A           |
| 2         | 0.244    | 1.98      | 99.0%        |
| 4         | 0.186    | 2.60      | 65.0%         |
| 6         | 0.194    | 2.49      | 41.5%         |
| 8         | 0.197    | 2.45      | 30.6%         |

Анализ результатов:

Анализ полученных результатов показывает следующие закономерности:

1. Масштабируемость алгоритма:
   - Режим task_run демонстрирует хорошее ускорение до 4 процессов, после чего прирост производительности замедляется
   - Режим task_pipeline показывает лучшее ускорение на 2 процессах, что близко к идеальному линейному ускорению
   - При увеличении количества процессов свыше 4 эффективность начинает заметно снижаться

2. Эффективность использования процессов:
   - Наибольшая эффективность достигается при использовании 2 процессов: 79.7% для task_run и 99.0% для task_pipeline
   - При 4 процессах эффективность снижается до 59.3% для task_run и 65.0% для task_pipeline
   - При 6 и 8 процессах эффективность падает до 30-40%, что указывает на преобладание накладных расходов над выигрышем от параллелизации

3. Сравнение режимов:
   - Режим task_pipeline показывает лучшую эффективность на малом количестве процессов 
   - Режим task_run демонстрирует более стабильную производительность при большом количестве процессов
   - Оба режима показывают сходные результаты при 6-8 процессах

4. Узкие места:
   - Накладные расходы на коммуникацию между процессами становятся значимыми при большом количестве процессов
   - Неравномерное распределение нагрузки может влиять на общую производительность
   - Синхронизация результатов через MPI_Bcast и MPI_Reduce добавляет задержки

Вывод: оптимальное количество процессов для данной задачи составляет 2-4, при котором достигается баланс между ускорением и эффективностью использования вычислительных ресурсов.

## 8. Выводы

В ходе выполнения работы были реализованы последовательный и параллельный алгоритмы численного интегрирования методом трапеций с использованием MPI.

Достижения:

- Реализован корректный последовательный алгоритм метода трапеций
- Разработана схема параллелизации с использованием MPI_Send и MPI_Reduce
- Обеспечена валидность работы на различных количествах процессов

Ограничения и проблемы:

- Производительность ограничена накладными расходами на коммуникацию между процессами
- Необходимость синхронизации результатов требует дополнительных коммуникаций

## 9. Источники

1. Open MPI Documentation. https://www.open-mpi.org/doc/
2. MPI Forum. MPI: A Message-Passing Interface Standard. Version 3.1
3. Numerical Methods for Engineers, Steven C. Chapra, Raymond P. Canale
4. Introduction to Parallel Computing, Ananth Grama, Anshul Gupta, George Karypis, Vipin Kumar

## Приложение

Пример ключевого фрагмента кода параллельного алгоритма:

```cpp
bool TochilinEIntegrationTrapezoidMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  std::array<double, 3> params = {0.0, 0.0, 0.0};
  if (rank == 0) {
    params[0] = lower_bound_;
    params[1] = upper_bound_;
    params[2] = static_cast<double>(num_intervals_);
  }

  MPI_Bcast(params.data(), 3, MPI_DOUBLE, 0, MPI_COMM_WORLD);

  double step = (params[1] - params[0]) / params[2];
  int base_intervals = static_cast<int>(params[2]) / size;
  int remainder = static_cast<int>(params[2]) % size;

  int local_start = 0;
  int local_count = 0;

  if (rank == 0) {
    local_start = 0;
    local_count = base_intervals + (rank < remainder ? 1 : 0);
    int current_pos = local_count;
    for (int proc = 1; proc < size; ++proc) {
      int count = base_intervals + (proc < remainder ? 1 : 0);
      std::array<double, 3> send_data = {
          static_cast<double>(current_pos),
          static_cast<double>(count),
          step};
      MPI_Send(send_data.data(), 3, MPI_DOUBLE, proc, 0, MPI_COMM_WORLD);
      current_pos += count;
    }
  } else {
    std::array<double, 3> recv_data = {0.0, 0.0, 0.0};
    MPI_Recv(recv_data.data(), 3, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD,
             MPI_STATUS_IGNORE);
    local_start = static_cast<int>(recv_data[0]);
    local_count = static_cast<int>(recv_data[1]);
    step = recv_data[2];
  }

  double local_sum = 0.0;
  for (int i = local_start; i < local_start + local_count; ++i) {
    double x_left = params[0] + (i * step);
    double x_right = params[0] + ((i + 1) * step);
    local_sum += ((function_(x_left) + function_(x_right)) / 2.0) * step;
  }

  double global_sum = 0.0;
  MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0,
             MPI_COMM_WORLD);

  if (rank == 0) {
    result_ = global_sum;
  }

  return true;
}
```

