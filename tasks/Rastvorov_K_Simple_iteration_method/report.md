# Метод простой итерации для решения СЛАУ

- Student: Растворов Кирилл Евгеньевич, группа 3823Б1ПР3
- Technology: SEQ, MPI
- Variant: 20

## 1. Introduction
В данной работе рассматривается задача численного решения системы линейных алгебраических уравнений (СЛАУ) методом простой итерации.  
Целью работы является реализация последовательной (SEQ) и параллельной (MPI) версий алгоритма, а также исследование их корректности и производительности.

Метод простой итерации выбран как базовый итерационный метод, наглядно демонстрирующий влияние распараллеливания на вычислительную эффективность.

## 2. Problem Statement
Требуется решить систему линейных уравнений вида:

\[
A x = b
\]

где:
- \( A \in \mathbb{R}^{n \times n} \) — квадратная матрица специального вида:
  - \( A_{ii} = 2n \)
  - \( A_{ij} = 1 \), при \( i \neq j \)
- \( b \in \mathbb{R}^n \), где \( b_i = 1 \)
- \( n \) — размер системы (входной параметр задачи)

Выходные данные — вектор решения \( x \in \mathbb{R}^n \).

Ограничения:
- \( n > 0 \)
- точность решения определяется заданным порогом \( \varepsilon \)

## 3. Baseline Algorithm (Sequential)
Для решения СЛАУ используется метод Якоби (метод простой итерации):

\[
x_i^{(k+1)} = \frac{1}{A_{ii}} \left( b_i - \sum_{j \neq i} A_{ij} x_j^{(k)} \right)
\]

С учётом специального вида матрицы \( A \) формула упрощается:

\[
x_i^{(k+1)} = \frac{1 - \sum x^{(k)} + x_i^{(k)}}{2n}
\]

Алгоритм итеративно обновляет вектор решения до выполнения условия сходимости:

\[
\max_i |x_i^{(k+1)} - x_i^{(k)}| < \varepsilon
\]

Последовательная версия реализует данный алгоритм за \( O(n) \) операций на одну итерацию.

## 4. Parallelization Scheme
### MPI
Для параллельной реализации используется модель MPI с распределением данных по процессам.

- Вектор решения \( x \) разбивается на непрерывные блоки
- Каждый процесс вычисляет свою локальную часть вектора
- Для вычисления суммы всех элементов используется коллективная операция `MPI_Allreduce`
- Для проверки критерия сходимости также используется `MPI_Allreduce`
- Финальный вектор собирается на процессе с рангом 0 с помощью `MPI_Gatherv`

Все процессы участвуют в итерациях синхронно.

## 5. Implementation Details
- Структура проекта:
  - `common/include/common.hpp` — общие типы и BaseTask
  - `seq/` — последовательная реализация
  - `mpi/` — параллельная реализация
- Ключевые классы:
  - `RastvorovKSimpleIterationMethodSEQ`
  - `RastvorovKSimpleIterationMethodMPI`
- Обрабатываются крайние случаи:
  - \( n \le 0 \)
- Используется фиксированное число итераций с досрочным выходом по критерию сходимости

## 6. Experimental Setup
- Hardware/OS:
  - CPU: 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz 
  - RAM: 8 ГБ
  - OS: Windows 10
- Toolchain:
  - Compiler: MSVC / clang
  - C++ standard: C++20
  - Build type: Release
- Data:
  - Размер системы задаётся параметрами тестов

mpiexec -n 2 .\ppc_func_tests.exe --gtest_filter=SimpleIterTests/RastvorovKRunFuncTestsProcesses.*
mpiexec -n 2 .\ppc_perf_tests.exe --gtest_filter=RunModeTests/RastvorovKSimpleIterationPerfTestProcesses.*

## 7. Results and Discussion

### 7.1 Correctness
Корректность реализации проверялась:
- функциональными тестами (SEQ и MPI)
- сравнением результатов с эталонной реализацией
- проверкой сходимости алгоритма

Все тесты были успешно пройдены.

### 7.2 Performance
Ниже приведены результаты измерения времени выполнения, ускорения и эффективности для последовательной (SEQ) и параллельной (MPI) реализаций.
   Pipeline
| Processes | Time (s) | Speedup vs SEQ | Efficiency |
|----------:|---------:|---------------:|-----------:|
| SEQ (1)   | 0.002435 | 1.00×          | 100%       |
| MPI 1     | 0.002338 | 1.04×          | 104%       |
| MPI 2     | 0.002192 | 1.11×          | 56%        |
| MPI 4     | 0.001270 | 1.92×          | 48%        |
    Tusk Run
| Processes | Time (s) | Speedup vs SEQ | Efficiency |
|----------:|---------:|---------------:|-----------:|
| SEQ (1)   | 0.002402 | 1.00×          | 100%       |
| MPI 1     | 0.002187 | 1.10×          | 110%       |
| MPI 2     | 0.001189 | 2.02×          | 101%       |
| MPI 4     | 0.001300 | 1.85×          | 46%        |

MPI-реализация демонстрирует ускорение по сравнению с последовательной версией.
Для режима Task Run наибольшее ускорение достигается при использовании двух процессов
(≈2.02×). При увеличении числа процессов эффективность снижается из-за накладных
расходов на обмен данными и синхронизацию в MPI.

## 8. Conclusions
В рамках работы были реализованы последовательная и параллельная версии метода простой итерации для решения СЛАУ.  
MPI-реализация показала значительное ускорение по сравнению с SEQ-версией при сохранении корректности результатов.

Основным ограничением является необходимость синхронизации на каждой итерации, что может снижать масштабируемость при большом числе процессов.

## 9. References
1. OpenMPI Documentation — https://www.open-mpi.org/doc/
2. PPC Parallel Programming Course — https://learning-process.github.io/parallel_programming_course/
3. GTest Framework — https://github.com/google/googletest
4. Статья https://www.cyberforum.ru/cpp-beginners/thread597195.html
5. Книга "Параллельное программированиена C++ в действии" автор: ANTHONY WILLIAMS https://dodo.inm.ras.ru/konshin/HPC/bib/HPC-cxx11-book.pdf
6. Физическая книга С.Макконнел "Совершенный код. Практическое руководство по разработке программного Обеспечения" 2014г.

## Appendix (Optional)
```cpp
// Формула итерации:
// x_i = (1 - sum(x) + x_i) / (2 * n)
