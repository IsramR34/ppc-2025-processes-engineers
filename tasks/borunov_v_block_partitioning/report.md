# Линейная фильтрация изображений (блочное разбиение). Ядро Гаусса 3x3.

- Student: **Борунов Владислав Алексеевич**, group **3823Б1ПР3**
- Technology: **SEQ | MPI**
- Variant: **28**

## 1. Введение

Работа посвящена реализации и анализу алгоритма блочного разбиения изображения и применения 3x3 фильтра к изображению. Цель — реализовать последовательную и MPI-реализацию пространственной фильтрации, корректно распределить работу по процессам, обеспечить согласованный результат и проанализировать поведение по времени.

## 2. Постановка задачи

Задача:

Для заданных параметров входа ширина width, высота height и массив пикселей pixels необходимо применить 3x3 свёрточный фильтр к изображению с учетом граничных условий защипывание индексов, вернуть результирующий массив пикселей в виде вектора std::vector<int>.

Формат I/O:

Вход (I): вектор целых чисел InType формата { width, height, pix0, pix1, ... }.

Выход (O): std::vector<int> длиной width * height.

## 3. Последовательная реализация (SEQ)

Описание алгоритма:

Последовательная реализация обходит каждый пиксель изображения и применяет 3x3 ядро:

```cpp
const std::array<std::array<float, 3>, 3> kernel = {{
  {1.0F / 16.0F, 2.0F / 16.0F, 1.0F / 16.0F},
  {2.0F / 16.0F, 4.0F / 16.0F, 2.0F / 16.0F},
  {1.0F / 16.0F, 2.0F / 16.0F, 1.0F / 16.0F},
}};

for (int i = 0; i < height; ++i) {
  for (int j = 0; j < width; ++j) {
    const int x0 = std::clamp(j - 1, 0, width - 1);
    const int x1 = j;
    const int x2 = std::clamp(j + 1, 0, width - 1);

    const int y0 = std::clamp(i - 1, 0, height - 1);
    const int y1 = i;
    const int y2 = std::clamp(i + 1, 0, height - 1);

    float sum = 0.0F;
    sum += pixels[(y0 * width) + x0] * kernel[0][0];
    // ... 
    output[(i * width) + j] = static_cast<int>(std::round(sum));
  }
}
```

Граничные условия: для краевых пикселей используются ближайшие доступные индексы, чтобы избежать обращения за пределы.

## 4. Схема распараллеливания (MPI)

Идея распараллеливания:

- Разбиение выполняется построчно: каждая MPI-ранк получает набор строк. Распределение учитывает остаток remainder и использует неравномерное разбиение, чтобы строки распределялись равномерно по ранкам.
- Вместо рассылки всего изображения каждому ранку, реализация теперь посылает каждому процессу только ту часть изображения, которая ему нужна для вычислений: его собственные строки плюс по одной halo строке сверху и снизу при наличии соседей. Это обеспечивает корректность свёртки на границах блока.
- Каждый процесс вычисляет фильтр только на своём наборе строк, а результаты собираются на ранке 0 с помощью MPI_Gatherv.

Коммуникационные операции:

- MPI_Scatterv - рассылка от ранка 0 для каждого процесса блока пикселей, включающего необходимые halo-строки.
- MPI_Gatherv - сбор локальных частей результата на ранке 0 с учётом send_counts и displs.
- MPI_Bcast - используется для рассылки размеров изображения width, height и для финального бродкаста результирующего массива.

Особенности:

- Отправка только необходимых блоков с halo-строками уменьшает объём передаваемых данных и снижает требования к памяти для каждого процесса по сравнению с рассылкой всего изображения.
- Появляется небольшая дополнительная логика: расчёт send/recv смещений с учётом halo, корректная обработка краевых ранков (которые не имеют соседей сверху/снизу) и гарантии, что каждый процесс получает ровно те строки, которые нужны для локальной свёртки.

## 5. Детали реализации

### 5.1 Структура кода

Файлы:

- `common/include/common.hpp` - общие типы данных.
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` - последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` - параллельная реализация
- `tests/functional/main.cpp` - функциональные тесты
- `tests/performance/main.cpp` - тесты производительности

Ключевые методы:

- ValidationImpl() - проверка корректности входных данных.
- PreProcessingImpl() - выделение выхода и начальная инициализация на ранке 0.
- RunImpl() - основная логика: распределение строк, локальная фильтрация.
- PostProcessingImpl() - восстановление результата на всех ранках через MPI_Bcast.

### 5.2 Важные проектные решения


- Рассылка только необходимых блоков пикселей с добавлением по одной halo-строке сверху/снизу с помощью MPI_Scatterv, чтобы обеспечить корректную локальную свёртку на границах блоков.

- Сбор через MPI_Gatherv локальные результаты собираются без halo-строк.

- Обработка граничных пикселей: применяется std::clamp, при этом при расчёте halo учитываются краевые ранки, у которых отсутствуют вышележащая или нижележащая строки.

- Проверка корректности: результаты приводятся к int через std::round.


## 6. Экспериментальная установка

### Аппаратное обеспечение

- CPU: i7-12650H 
- Ядра: 10
- Потоки: 16
- ОЗУ: 16 ГБ
- ОС: Windows 11

### Программное обеспечение

- Компилятор: MSVC 14.44
- Сборка: Release 

### Генерация тестовых сценариев

- Функциональные тесты запускаются для небольших размеров (10x10, 20x15, 32x32) с фиксированным seed для сравнения результата.
- Тест производительности использует большие изображения 4000x4000 для измерения масштабируемости.

## 7. Результаты и обсуждение

### 7.1 Корректность 

- Функциональные тесты проходят успешно: результат MPI-реализации эквивалентен последовательной эталонной реализации output == reference_output_.
- Пограничные случаи корректно обрабатываются благодаря clamp.

### 7.2 Производительность 

| Режим | Процессов | Время, сек | Ускорение | Эффективность |
|-------|-----------|------------|-----------|---------------|
| seq   | 1         | 1.8893     | 1.00      | N/A           |
|-------|-----------|------------|-----------|---------------|
| mpi   | 2         | 0.9571     | 1.974     | 98.7%         |
|-------|-----------|------------|-----------|---------------|
| mpi   | 4         | 0.4879     | 3.872     | 96.8%         |
|-------|-----------|------------|-----------|---------------|
| mpi   | 7         | 0.3578     | 5.281     | 75.4%         |
|-------|-----------|------------|-----------|---------------|
| mpi   | 8         | 0.3410     | 5.542     | 69.3%         |
|-------|-----------|------------|-----------|---------------|
| mpi   | 32        | 0.3638     | 5.193     | 16.2%         |
|-------|-----------|------------|-----------|---------------|

- MPI-реализация демонстрирует ускорение для больших изображений при умеренном числе процессов.


## 8. Conclusions

## Результаты:

- Реализована корректная и надёжная схема обработки граничных областей между блоками: используется рассылка только необходимых блоков с дополнительными halo-строками MPI_Scatterv и std::clamp для корректной обработки граничных пикселей, что уменьшает объём передаваемых данных и требования к памяти.

- Функциональные тесты проходят успешно, подтверждая эквивалентность MPI-реализации и последовательной эталонной реализации. Для точного количественного вывода о влиянии изменения коммуникации на производительность требуется повторный замер.


## Ограничения и проблемы

- Отправка только необходимых блоков уменьшает требования по памяти и объём передаваемых данных, но добавляет небольшую сложность в расчётах send_counts_halo/displs_halo и в обработке краевых ранков.


## 9. Ссылки

- Microsoft. Microsoft MPI Documentation. https://learn.microsoft.com/en-us/message-passing-interface/microsoft-mpi
- Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.

## Appendix

```cpp
const std::array<std::array<float,3>,3> kernel = {{
  {1.0f/16.0f, 2.0f/16.0f, 1.0f/16.0f},
  {2.0f/16.0f, 4.0f/16.0f, 2.0f/16.0f},
  {1.0f/16.0f, 2.0f/16.0f, 1.0f/16.0f},
}};

const int* pixels = input.data() + 2;
std::vector<int> output(width * height);

for (int y = 0; y < height; ++y) {
  for (int x = 0; x < width; ++x) {
    const int x0 = std::clamp(x - 1, 0, width - 1);
    const int x1 = x;
    const int x2 = std::clamp(x + 1, 0, width - 1);

    const int y0 = std::clamp(y - 1, 0, height - 1);
    const int y1 = y;
    const int y2 = std::clamp(y + 1, 0, height - 1);

    float sum = 0.0f;
    sum += pixels[y0*width + x0] * kernel[0][0];
    sum += pixels[y0*width + x1] * kernel[0][1];
    sum += pixels[y0*width + x2] * kernel[0][2];

    sum += pixels[y1*width + x0] * kernel[1][0];
    sum += pixels[y1*width + x1] * kernel[1][1];
    sum += pixels[y1*width + x2] * kernel[1][2];

    sum += pixels[y2*width + x0] * kernel[2][0];
    sum += pixels[y2*width + x1] * kernel[2][1];
    sum += pixels[y2*width + x2] * kernel[2][2];

    output[y*width + x] = static_cast<int>(std::round(sum));
  }
}
```

1) расчёт распределения строк и смещений

```cpp
int rows_per_proc = height / size;
int remainder = height % size;
std::vector<int> send_counts(size), displs(size);
int offset = 0;
for (int r = 0; r < size; ++r) {
  int rows = rows_per_proc + (r < remainder ? 1 : 0);
  send_counts[r] = rows * width; 
  displs[r] = offset;
  offset += send_counts[r];
}

int my_pixels = send_counts[rank];
int my_rows = my_pixels / width;
int row_start = std::accumulate(send_counts.begin(), send_counts.begin() + rank, 0) / width;
```

2) подготовка буферов с halo вариант с рассылкой блоков, включающих halo:

```cpp
std::vector<int> send_counts_halo(size), displs_halo(size);
offset = 0;
for (int r = 0; r < size; ++r) {
  int rows = send_counts[r] / width;
  int rows_with_halo = rows;
  if (r > 0) rows_with_halo += 1;         
  if (r + 1 < size) rows_with_halo += 1;  
  send_counts_halo[r] = rows_with_halo * width;
  displs_halo[r] = offset;
  offset += send_counts_halo[r];
}

MPI_Scatterv(rank == 0 ? full_with_halo.data() : nullptr,
             send_counts_halo.data(), displs_halo.data(), MPI_INT,
             local_with_halo.data(), send_counts_halo[rank], MPI_INT,
             0, MPI_COMM_WORLD);
```



