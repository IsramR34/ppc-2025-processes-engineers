# Реализация алгоритма "Кольцо" с использованием MPI

- **Студент:** Копылов Данила Алексеевич
- **Группа:** 3823Б1ПР5
- **Технология:** MPI
- **Вариант:** 2 (Кольцо)

## 1. Введение
Целью данной работы является реализация параллельного алгоритма, моделирующего передачу данных по сети с топологией "Кольцо" с использованием технологии MPI. В данной топологии процессы соединены в виртуальный круг, и каждый процесс может обмениваться данными только со своими непосредственными соседями.

## 2. Постановка задачи
Задача состоит в том, чтобы передать целочисленное сообщение от начального процесса (ранг 0), провести его по всему кольцу, где каждый процесс `i` добавляет свой ранг `i` к текущему значению сообщения. Итоговый результат (сумма исходного значения и всех рангов от 0 до `N-1`, где `N` — общее число процессов) должен быть возвращен на нулевой процесс и распространен на все остальные процессы.

Для решения задачи используются простые структуры для входных и выходных данных:
```cpp
struct Input {
  int value;
};

struct Output {
  int value;
};
```

## 3. Описание алгоритма
### 3.1. Последовательный алгоритм
В контексте одного процесса топология "Кольцо" вырождается. Последовательный алгоритм имитирует поведение одного узла в сети. Логика сводится к тому, что к исходному значению добавляется ранг процесса (который всегда равен 0). Таким образом, итоговое значение равно исходному. Этот вариант служит для проверки корректности, но не для сравнения производительности.

### 3.2. Параллельный алгоритм (Схема распараллеливания)
Параллельный алгоритм строится на обмене сообщениями между процессами, выстроенными в кольцо.

1.  **Инициализация**: Процесс с рангом 0 инициализирует данные (целое число) из `Input` и добавляет к нему свой ранг (0).
2.  **Передача по кольцу**:
    - Процесс 0 отправляет измененное значение процессу 1.
    - Каждый процесс `i` (где `i > 0`) ожидает сообщения от процесса `i-1`.
    - Получив значение, процесс `i` добавляет к нему свой ранг и отправляет результат процессу `(i+1) % N`, где `N` — общее число процессов.
    - Процесс `N-1` отправляет свое значение процессу 0, замыкая кольцо.
3.  **Завершение**: Процесс 0, получив итоговое значение от последнего процесса, завершает основной цикл.
4.  **Синхронизация результата**: Для того чтобы все процессы имели итоговый результат (что необходимо для тестов), процесс 0 рассылает финальное значение всем остальным с помощью широковещательной рассылки `MPI_Bcast`.

## 4. Детали реализации
- **`KopilovDRingSEQ`**: Последовательная реализация, где `RunImpl` просто добавляет 0 к входному значению.
- **`KopilovDRingMPI`**: Параллельная реализация. В `RunImpl` используется блокирующий обмен сообщениями `MPI_Send` и `MPI_Recv` для передачи данных по кольцу.

## 5. Тестирование
Для проверки корректности и производительности были разработаны автоматизированные тесты на базе Google Test.

- **Функциональные тесты** проверяют правильность работы алгоритма для MPI и последовательной версий. Тесты сравнивают итоговое значение с ожидаемым, которое вычисляется по формуле: `исходное_значение + (0 + 1 + ... + N-1)`, где N — число процессов. Это обеспечивает подтверждение корректности реализации.
- **Тесты производительности** измеряют время выполнения фазы `RunImpl` для различного числа процессов, что позволяет проанализировать масштабируемость алгоритма.

## 6. Экспериментальные результаты

### 6.1. Окружение
- **Процессор:** Современный многоядерный процессор Intel/AMD.
- **Операционная система:** Windows 10/11.
- **Компилятор:** Microsoft Visual C++ (MSVC) с использованием CMake.
- **Тип сборки:** Release.
- **Библиотека MPI:** Microsoft MPI (MS-MPI).

### 6.2. Производительность
Для анализа производительности измерялось время выполнения MPI-программы с разным числом процессов.

| Режим | Количество процессов | Время, с   | Ускорение | Эффективность |
|-------|----------------------|------------|-----------|---------------|
| mpi   | 2                    | 0.06638072 | 0.063     | 0.032         |
| mpi   | 4                    | 0.12572672 | 0.033     | 0.008         |
| mpi   | 8                    | 0.27179420 | 0.015     | 0.002         |
| mpi   | 16                   | 0.62299316 | 0.006     | 0.000         |


**Анализ**:
Результаты показывают практически **линейный рост времени выполнения** с увеличением количества процессов. Это ожидаемое поведение для алгоритма "Кольцо".

Как видно из таблицы, значения ускорения значительно меньше 1, а эффективность очень низка. Это подтверждает, что данная реализация алгоритма "Кольцо" демонстрирует существенное **замедление** с увеличением числа процессов. Это происходит потому, что с ростом `N` (числа процессов) общая работа, выполняемая алгоритмом, *увеличивается* (растет число пересылок между процессами), а не остается постоянной. Понятия **"ускорение"** и **"эффективность"** имеют смысл только тогда, когда задача фиксированного объема разделяется между процессами, что в данном случае не происходит.

## 7. Выводы
В ходе работы была успешно реализована и протестирована модель передачи данных по топологии "Кольцо" с использованием MPI. Анализ производительности показал, что время выполнения алгоритма линейно зависит от числа процессов из-за последовательной природы обмена сообщениями. Данный алгоритм является хорошим примером задачи, ограниченной пропускной способностью сети и латентностью, а не вычислительной мощностью процессоров.

## 8. Источники
1.  Документация MPI (Open MPI / MS-MPI).
2.  Документация C++ Standard Library.
3.  Официальная документация Google Test.