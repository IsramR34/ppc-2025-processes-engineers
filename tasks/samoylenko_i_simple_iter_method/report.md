# Метод простой итерации

-   Студент: Самойленко Илья Андреевич, группа 3823Б1ПР3
-   Технология: SEQ|MPI
-   Вариант: 20

## 1. Введение

Задача состоит в реализации последовательной и параллельной версий метода простой итерации для решения системы линейных алгебраических уравнений, а также создание функциональных тестов для проверки корректности решения и тестов на производительность для сравнения последовательной и параллельной реализаций метода.

## 2. Постановка задачи

Дана система линейных уравнений Ax = b.
Необходимо найти вектор x, удовлетворяющий уравнению с заданной точностью $\epsilon$.
В этой задаче в качестве метода простой итерации был выбран метод Ричардсона. Далее в отчёте в качестве метода простой итерации будет описываться именно этот вариант.

Матрица A (матрица коэффициентов) генерируется следующим образом:

-   Элементы главной диагонали равны 4.0.
-   Элементы побочных диагоналей равны 1.0.
-   Остальные элементы равны 0.0.

Вектор b (вектор свободных членов) инициализируется значениями 1.0.

Метод простой итерации заключается в итеративном вычислении нового приближения вектора x по формуле:
$$x^{(k+1)} = x^{(k)} - \tau (Ax^{(k)} - b)$$
где $\tau$ - итерационный параметр.

Метод сходится при выполнении условия $0 < \tau < \frac{2}{\rho(A)}$, где $\rho(A)$ - спектральный радиус матрицы A.

Выход из цикла итераций происходит при выполнении условия $\lVert x^{(k+1)} - x^{(k)} \rVert_\infty < \epsilon$ или достижении максимального количества итераций.

В данной реализации:

-   $\tau = 0.2$ (удовлетворяет условию сходимости)
-   $\epsilon = 10^{-7}$
-   Максимальное количество итераций = 2000
-   Начальное приближение $x^{(0)}$ - нулевой вектор.

Входные данные: размер матрицы N.

Выходные данные: вектор решения x.

## 3. Базовый алгоритм (Последовательный)

Алгоритм реализует классическую схему метода простой итерации.
На каждой итерации вычисляется невязка и считается вектор решения.

```cpp
for (int it = 0; it < iters; ++it) {
  for (size_t i = 0; i < size; ++i) {
    double ax_i = 0.0;
    for (size_t j = 0; j < size; ++j) {
      ax_i += matrix[(i * size) + j] * x_old[j];
    }
    x_new[i] = x_old[i] - (tau * (ax_i - vector[i]));
  }
}
```

Проверка сходимости осуществляется путем поиска максимальной разности между компонентами векторов на текущей и предыдущей итерациях.

```cpp
double max_diff = 0.0;
for (size_t i = 0; i < size; ++i) {
  max_diff = std::max(max_diff, std::fabs(x_new[i] - x_old[i]));
}

x_old.swap(x_new);

if (max_diff < eps) {
  break;
}
```

## 4. Схема распараллеливания

Матрица A и векторы b, x_new распределяются между процессами.
Вектор x_old (предыдущее приближение) дублируется на всех процессах, так как для вычисления части $Ax^{(k)}$ каждому процессу необходим полный вектор $x^{(k)}$.

На каждой итерации каждый процесс вычисляет свою часть вектора x_new, после чего происходит сборка полного вектора x_new на всех процессах для следующей итерации.

```cpp
MPI_Allgatherv(local_x_new.data(), local_rows, MPI_DOUBLE, x.data(), row_counts.data(), row_displs.data(), MPI_DOUBLE, MPI_COMM_WORLD);
```

Вычисляется локальная максимальная разность, затем находится глобальный максимум разности для проверки критерия остановки.

```cpp
MPI_Allreduce(&local_max, &global_max, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);
```

## 5. Детали реализации

Структура кода

```
tasks/samoylenko_i_simple_iter_method/
├───common
│   └───include
│           common.hpp // Определение типов данных
├───mpi
│   ├───include
│   │       ops_mpi.hpp
│   └───src
│           ops_mpi.cpp // Параллельная реализация
├───seq
│   ├───include
│   │       ops_seq.hpp
│   └───src
│           ops_seq.cpp // Последовательная реализация
└───tests
    ├───functional
    │       main.cpp // Функциональные тесты
    └───performance
            main.cpp // Тесты на производительность
```

Краевые случаи:

-   Нулевой или отрицательный размер системы (N <= 0).
-   Количество процессов больше размера системы (world_size > N)
-   Минимальный размер системы (N = 1)

Особенности использования памяти:

-   В параллельной версии каждый процесс хранит только свою строку матрицы A и весь вектор x.

## 6. Экспериментальная настройка

Аппаратное обеспечение и ОС:

-   CPU: AMD Ryzen 5 2600X
-   ядра: 6
-   потоки: 12
-   RAM: 16 GB
-   OS: Ubuntu 24.04.2 LTS (DevContainer / Windows 10 22H2)

Набор инструментов:

-   Компилятор: g++ 13.3.0
-   Тип сборки: Release
-   MPI: OpenMPI 4.1.6

Переменные окружения:

-   OMPI_ALLOW_RUN_AS_ROOT=1
-   OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1

Данные: Для тестов на производительность размер системы: N = 3000.

## 7. Результаты и обсуждение

### 7.1 Корректность

Функциональные тесты проверяют корректность вычисления решения путем подстановки найденного вектора x в исходную систему уравнений и вычисления невязки $r = Ax - b$. Критерием корректности является выполнение условия $\lVert r \rVert_\infty < \epsilon$. Все тесты проходят успешно. Отдельно тестируются краевые случаи для обеих реализаций метода.

### 7.2 Производительность

| Режим | Процессы | Время, с | Ускорение | Эффективность |
| ----- | -------- | -------- | --------- | ------------- |
| seq   | 1        | 1.17     | 1.00      | N/A           |
| mpi   | 1        | 1.17     | 1.00      | 100%          |
| mpi   | 2        | 0.60     | 1.95      | 97.5%         |
| mpi   | 4        | 0.32     | 3.65      | 91.25%        |

Ускорение = Время seq / Время mpi

Эффективность = Ускорение / Процессы * 100%

## 8. Заключение

Реализован метод простой итерации для решения СЛАУ. MPI-версия использует декомпозицию данных по строкам. Обе реализации прошли все тесты на функционал. MPI-версия масштабируется корректно. В данной задаче затраты на коммуникацию между процессами повлияли на эффективность и ускорение лишь незначительно в силу большого объема вычислений.

## 9. Источники

1. Метод Ричардсона: https://en.wikipedia.org/wiki/Modified_Richardson_iteration
2. Материалы курса: https://learning-process.github.io/parallel_programming_course/ru/index.html
3. Документация Open MPI: https://www-lb.open-mpi.org/doc/

## Приложение

```cpp
  int world_rank = 0;
  int world_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  int n = 0;
  if (world_rank == 0) {
    n = GetInput();
  }

  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (n <= 0) {
    return false;
  }

  std::vector<int> row_counts(world_size);
  std::vector<int> row_displs(world_size);
  CalculateDistribution(n, world_size, row_counts, row_displs);

  int local_rows = row_counts[world_rank];
  int local_start = row_displs[world_rank];

  std::vector<double> local_matrix = BuildLocalMatrix(n, local_rows, local_start);
  std::vector<double> local_vector(local_rows, 1.0);
  std::vector<double> x(static_cast<size_t>(n), 0.0);

  PerformIterations(n, local_rows, local_start, local_matrix, local_vector, row_counts, row_displs, x);

  if (world_rank == 0) {
    GetOutput() = x;
  }

  return true;
```